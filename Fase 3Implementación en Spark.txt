 TAREA 3 - Procesamiento en Batch con Spark
# Requisitos:
# 1) Cargar dataset desde la fuente original (HDFS)
# 2) Limpiar / transformar + EDA (DataFrames y mini ejemplo RDD)
# 3) Almacenar resultados procesados en HDFS
# ===============================================

from pyspark.sql import SparkSession, functions as F, types as T

# ---------- 0) SesiÃ³n Spark ----------
spark = SparkSession.builder.appName("Tarea3_Batch").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

# ---------- 1) Carga del conjunto de datos ----------
INPUT_PATH = "hdfs://localhost:9000/Tarea3/rows.csv"

# Salidas (usa tu home en HDFS para evitar AccessControlException)
OUT_BASE = "hdfs://localhost:9000/user/vboxuser/Tarea3/output_batch"
OUT_CLEAN_PARQUET = f"{OUT_BASE}/clean_parquet"
OUT_EDA_NIVEL_CSV = f"{OUT_BASE}/eda_nivel_csv"
OUT_EDA_HAB_CSV   = f"{OUT_BASE}/eda_habilitado_csv"
OUT_TOP3_PARQUET  = f"{OUT_BASE}/ips_top3_parquet"
OUT_ORD_PARQUET   = f"{OUT_BASE}/ips_ordenadas_parquet"

df = (
    spark.read.format("csv")
    .option("header", "true")
    .option("inferSchema", "true")   # infiere tipos de columnas
    .load(INPUT_PATH)
)

print("\nğŸ“˜ Esquema inicial:")
df.printSchema()

print("\nğŸ“Š Primeras 10 filas:")
df.show(10, truncate=False)

# ---------- 2) Limpieza y TransformaciÃ³n ----------
# Normaliza textos, castea tipos y convierte fechas (YYYYMMDD -> date)
# Asegura enteros en 'nivel' (algunos vienen nulos)
df2 = df.withColumn("nivel", F.col("nivel").cast("int"))

# Normaliza habilitado a mayÃºsculas
df2 = df2.withColumn("habilitado", F.upper(F.col("habilitado")))

# Convierte fechas en entero YYYYMMDD a date
def yyyymmdd_to_date(col):
    # castea a string, rellena a 8, y convierte con formato
    return F.to_date(F.lpad(F.col(col).cast("string"), 8, "0"), "yyyyMMdd")

for c in ["fecha_radicacion", "fecha_vencimiento", "fecha_cierre"]:
    df2 = df2.withColumn(c + "_date", yyyymmdd_to_date(c))

# Elimina duplicados y filas sin datos crÃ­ticos
df2 = df2.dropDuplicates()
df2 = df2.dropna(subset=["nombre_prestador", "habilitado"])  # nivel puede faltar; no filtramos aÃºn

print("\nğŸ“˜ Esquema despuÃ©s de limpieza:")
df2.printSchema()

# ---------- 3) EDA (DataFrames) ----------
print("\nğŸ“ˆ EstadÃ­sticas descriptivas (nivel):")
df2.describe(["nivel"]).show()

print("\nğŸ”¢ DistribuciÃ³n por niveles de atenciÃ³n:")
eda_nivel = df2.groupBy("nivel").count().orderBy("nivel")
eda_nivel.show()

print("\nğŸ¥ Conteo por estado de habilitaciÃ³n:")
eda_hab = df2.groupBy("habilitado").count().orderBy(F.desc("count"))
eda_hab.show()

# Filtro de interÃ©s: IPS habilitadas y nivel >= 3
print("\nğŸ” IPS habilitadas con nivel >= 3:")
ips_filtradas = (
    df2.filter((F.col("habilitado") == "SI") & (F.col("nivel") >= 3))
       .select(
           "nombre_prestador", "depa_nombre", "muni_nombre",
           "nivel", "fecha_vencimiento_date"
 )
       .withColumnRenamed("fecha_vencimiento_date", "fecha_vencimiento")
)
ips_filtradas.show(10, truncate=False)

# Orden por fecha de vencimiento (mÃ¡s recientes primero)
print("\nğŸ“… IPS ordenadas por fecha de vencimiento (desc):")
ips_ordenadas = ips_filtradas.orderBy(F.col("fecha_vencimiento").desc_nulls_last())
ips_ordenadas.show(10, truncate=False)

# Top 3 por departamento segÃºn fecha de vencimiento mÃ¡s reciente
w_dep = (
    F.row_number()
    .over(
        Window.partitionBy("depa_nombre")
              .orderBy(F.col("fecha_vencimiento").desc_nulls_last())
    )
)
from pyspark.sql import Window
ips_top3_dep = (
    ips_filtradas
    .withColumn("rn", w_dep)
    .filter(F.col("rn") <= 3)
    .drop("rn")
)
print("\nğŸ¥‡ Top 3 IPS por departamento (por fecha de vencimiento):")
ips_top3_dep.show(30, truncate=False)

# ---------- 4) Mini EDA con RDD (opcional, para cubrir 'RDDs o DataFrames') ----------
print("\nğŸ§± Ejemplo RDD: nÃºmero de municipios distintos por departamento (mÃ©todo RDD)")
rdd_dep_muni = (
    df2.select("depa_nombre", "muni_nombre")
       .dropna()
       .rdd
       .map(lambda r: ((r["depa_nombre"], r["muni_nombre"]), 1))
       .reduceByKey(lambda a, b: a + b)
       .map(lambda kv: (kv[0][0], 1))          # (depa, 1) por municipio Ãºnico
       .reduceByKey(lambda a, b: a + b)        # (depa, num_municipios)
	   
	)
df_dep_muni = rdd_dep_muni.toDF(["depa_nombre", "municipios_unicos"])
df_dep_muni.orderBy(F.desc("municipios_unicos")).show(20, truncate=False)

# ---------- 5) Almacenamiento de resultados ----------
# Nota: Spark crearÃ¡ las carpetas si tienes permisos en /user/vboxuser.
# Si no existen, crea la base una sola vez:
#   hdfs dfs -mkdir -p /user/vboxuser/Tarea3/output_batch
#   sudo -u hdfs hdfs dfs -chown -R vboxuser:hadoop /user/vboxuser

print(f"\nğŸ’¾ Guardando dataset limpio en Parquet: {OUT_CLEAN_PARQUET}")
(
    df2.coalesce(1)
       .write.mode("overwrite")
       .option("compression", "snappy")
       .parquet(OUT_CLEAN_PARQUET)
)

print(f"ğŸ’¾ Guardando EDA por nivel (CSV): {OUT_EDA_NIVEL_CSV}")
(
    eda_nivel.coalesce(1)
             .write.mode("overwrite")
             .option("header", "true")
             .csv(OUT_EDA_NIVEL_CSV)
)

print(f"ğŸ’¾ Guardando EDA por habilitado (CSV): {OUT_EDA_HAB_CSV}")
(
    eda_hab.coalesce(1)
           .write.mode("overwrite")
           .option("header", "true")
           .csv(OUT_EDA_HAB_CSV)
)

print(f"ğŸ’¾ Guardando Top3 por departamento (Parquet): {OUT_TOP3_PARQUET}")
(
    ips_top3_dep.coalesce(1)
                .write.mode("overwrite")

	                .parquet(OUT_TOP3_PARQUET)
)

print(f"ğŸ’¾ Guardando IPS ordenadas por vencimiento (Parquet): {OUT_ORD_PARQUET}")
(
    ips_ordenadas.coalesce(1)
                 .write.mode("overwrite")
                 .option("compression", "snappy")
                 .parquet(OUT_ORD_PARQUET)
)

print("\nâœ… Proceso batch completado con Ã©xito.")
spark.stop()
			
